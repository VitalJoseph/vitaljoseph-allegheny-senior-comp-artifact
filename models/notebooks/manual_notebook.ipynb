{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09dc62e9-f9e3-4812-ba70-a26e51515045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prototype for manual categorization with Monte Carlo cross validation technique.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# File paths for the datasets\n",
    "file_paths_nfl = [\n",
    "    \"data/historical-nfl/2019wr.csv\",\n",
    "    \"data/historical-nfl/2020wr.csv\",\n",
    "    \"data/historical-nfl/2021wr.csv\",\n",
    "    \"data/historical-nfl/2022wr.csv\"\n",
    "]\n",
    "\n",
    "# Load and combine the datasets\n",
    "combined_data_nfl = pd.concat([pd.read_csv(path) for path in file_paths_nfl], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_path_nfl = \"data/historical-nfl/combined_wr_data.csv\"\n",
    "combined_data_nfl.to_csv(output_path_nfl, index=False)\n",
    "\n",
    "# Load and preprocess nfl stats\n",
    "nfl_stats = pd.read_csv(\"data/historical-nfl/combined_wr_data.csv\")\n",
    "nfl_stats['nflYears'] = nfl_stats['nflYears'].astype(float)\n",
    "\n",
    "# List of columns to normalize and average\n",
    "nfl_metrics_to_normalize = ['nflRec', 'nflYds', 'nflTD', 'AP1', 'St', 'PB']\n",
    "\n",
    "# Average the metrics by dividing by nflYears\n",
    "for metric in nfl_metrics_to_normalize:\n",
    "    nfl_stats[metric + '_avg'] = nfl_stats[metric] / nfl_stats['nflYears']\n",
    "\n",
    "# Standardize the averaged metrics\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Generate the list of column names for the normalized columns\n",
    "normalized = [metric + '_avg_normalized' for metric in nfl_metrics_to_normalize]\n",
    "\n",
    "# Standardize the averaged metrics (normalize by standard deviation)\n",
    "nfl_stats[normalized] = scaler.fit_transform(nfl_stats[[metric + '_avg' for metric in nfl_metrics_to_normalize]])\n",
    "\n",
    "# Standardize the un-averaged metrics\n",
    "nfl_stats['wAV_normalized'] = scaler.fit_transform(nfl_stats[['wAV']])\n",
    "\n",
    "# Define success metric using the normalized averaged metrics\n",
    "nfl_stats['SuccessMetric'] = (nfl_stats['nflYds_avg_normalized'] + nfl_stats['nflTD_avg_normalized'] +\n",
    "                               nfl_stats['nflRec_avg_normalized'] + nfl_stats['AP1_avg_normalized'] +\n",
    "                               nfl_stats['St_avg_normalized'] + nfl_stats['PB_avg_normalized'] +\n",
    "                               nfl_stats['wAV_normalized'])\n",
    "\n",
    "\n",
    "# File paths for the datasets\n",
    "file_paths_measurements = [\n",
    "    \"data/historical-measurements/2019wr.csv\",\n",
    "    \"data/historical-measurements/2020wr.csv\",\n",
    "    \"data/historical-measurements/2021wr.csv\",\n",
    "    \"data/historical-measurements/2022wr.csv\"\n",
    "]\n",
    "\n",
    "file_paths_combine = [\n",
    "    \"data/historical-combine/2019wr.csv\",\n",
    "    \"data/historical-combine/2020wr.csv\",\n",
    "    \"data/historical-combine/2021wr.csv\",\n",
    "    \"data/historical-combine/2022wr.csv\"\n",
    "]\n",
    "\n",
    "file_paths_college = [\n",
    "    \"data/historical-college/2019wr.csv\",\n",
    "    \"data/historical-college/2020wr.csv\",\n",
    "    \"data/historical-college/2021wr.csv\",\n",
    "    \"data/historical-college/2022wr.csv\"\n",
    "]\n",
    "\n",
    "# Load and combine the datasets\n",
    "combined_data_measurements = pd.concat([pd.read_csv(path) for path in file_paths_measurements], ignore_index=True)\n",
    "combined_data_combine = pd.concat([pd.read_csv(path) for path in file_paths_combine], ignore_index=True)\n",
    "combined_data_college = pd.concat([pd.read_csv(path) for path in file_paths_college], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_path_measurements = \"data/historical-measurements/combined_wr_data.csv\"\n",
    "combined_data_measurements.to_csv(output_path_measurements, index=False)\n",
    "\n",
    "output_path_combine = \"data/historical-combine/combined_wr_data.csv\"\n",
    "combined_data_combine.fillna(np.nan, inplace=True)\n",
    "combined_data_combine.to_csv(output_path_combine, index=False)\n",
    "\n",
    "output_path_college = \"data/historical-college/combined_wr_data.csv\"\n",
    "combined_data_college.to_csv(output_path_college, index=False)\n",
    "\n",
    "# Select features and target variable\n",
    "measurements = pd.read_csv(\"data/historical-measurements/combined_wr_data.csv\")\n",
    "combine_stats = pd.read_csv(\"data/historical-combine/combined_wr_data.csv\")\n",
    "college_stats = pd.read_csv(\"data/historical-college/combined_wr_data.csv\")\n",
    "\n",
    "conference_rankings = {\n",
    "    'SEC': 10,\n",
    "    'Big Ten': 9,\n",
    "    'Big 12': 8,\n",
    "    'Pac-12': 7,\n",
    "    'ACC': 6,\n",
    "    'AAC': 5,\n",
    "    'MWC': 4,\n",
    "    'C-USA': 3,\n",
    "    'Ind': 2,\n",
    "    'CAA': 1\n",
    "}\n",
    "\n",
    "# Map conference names to their rankings, default to 1 for any conference not listed\n",
    "college_stats['ConfRank'] = college_stats['Conf'].map(conference_rankings).fillna(1)\n",
    "\n",
    "# Merge datasets on Player column\n",
    "data = college_stats.merge(measurements, on=\"Player\").merge(combine_stats, on=\"Player\")\n",
    "\n",
    "# Ensure that the players in the target and features match\n",
    "merged_data = data.merge(nfl_stats[['Player', 'SuccessMetric']], on='Player', how='inner')\n",
    "\n",
    "# Features\n",
    "features = merged_data[['Rec', 'Yds', 'Y/R', 'TD', 'Y/G', 'G', 'ConfRank', '40yd', 'Height(in)', 'Weight', 'Hand(in)', 'Arm(in)', 'Wingspan(in)']]\n",
    "                        \n",
    "                        #, 'Height(in)', 'Weight', 'Hand(in)', 'Arm(in)', 'Wingspan(in)']]\n",
    "\n",
    "# Normalize the feature metrics (standard deviation normalization)\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the normalized features back into a DataFrame with the same column names\n",
    "features_normalized_df = pd.DataFrame(features_normalized, columns=features.columns)\n",
    "\n",
    "# Target\n",
    "target = merged_data['SuccessMetric']\n",
    "\n",
    "# Define categorize_player function\n",
    "def categorize_player(success_metric):\n",
    "    if success_metric >= 15:  \n",
    "        return \"All-Pro\"\n",
    "    elif success_metric >= 5:  \n",
    "        return \"Pro Bowler\"\n",
    "    elif success_metric >= 1:  \n",
    "        return \"Starter\"\n",
    "    elif success_metric >= -2:  \n",
    "        return \"Backup\"\n",
    "    else:\n",
    "        return \"Practice Squad\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0bca3de-13a9-470f-90df-ae3bc59ab8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Cross Validation\n",
    "num_iterations = 100\n",
    "\n",
    "mccv_results = defaultdict(list)\n",
    "player_success_scores = defaultdict(list)\n",
    "\n",
    "# Store the most common predicted and actual category per player\n",
    "player_actual_categories = defaultdict(list)\n",
    "player_predicted_categories = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb3dc84-3089-436a-81be-05828bb3ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.1, random_state=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4479697-f4ce-41ac-a335-948b2a55d021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11818bc4-25a4-462b-8951-9b90acee42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Predict success scores\n",
    "    predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd432b7e-dcc6-41fe-b294-a8fdc8f3a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Store success scores for each player\n",
    "    for player, score in zip(merged_data.loc[X_test.index, 'Player'], predictions):\n",
    "        player_success_scores[player].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8b71d-9219-4305-897d-7928132fac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Categorize predictions\n",
    "    predicted_categories = [categorize_player(pred) for pred in predictions]\n",
    "    actual_categories = [categorize_player(actual) for actual in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d9f6b-12c8-492e-8a42-3d659f22639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Store the category predictions for each player\n",
    "    for player, actual, predicted in zip(merged_data.loc[X_test.index, 'Player'], actual_categories, predicted_categories):\n",
    "        player_actual_categories[player].append(actual)\n",
    "        player_predicted_categories[player].append(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24c1a6-38c1-412b-860b-a83d5a112a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Store the accuracy and category counts\n",
    "    mccv_results['accuracy'].append(\n",
    "        sum(p == a for p, a in zip(predicted_categories, actual_categories)) / len(y_test)\n",
    "    )\n",
    "    mccv_results['predicted_categories'].append(player_predicted_categories)\n",
    "    mccv_results['actual_categories'].append(player_actual_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fee19-5452-4752-818c-d94718ddc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "mean_accuracy = np.mean(mccv_results['accuracy'])\n",
    "std_accuracy = np.std(mccv_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cab5b6-dcb1-4dbf-8a29-31d935ea0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Monte Carlo Cross-Validation Report ===\")\n",
    "print(f\"\\nMean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Aggregate category results\n",
    "final_actual_categories = {player: Counter(categories).most_common(1)[0][0] for player, categories in player_actual_categories.items()}\n",
    "final_predicted_categories = {player: Counter(categories).most_common(1)[0][0] for player, categories in player_predicted_categories.items()}\n",
    "\n",
    "# Compute average success score per player\n",
    "average_predicted_success_scores = {player: np.mean(scores) for player, scores in player_success_scores.items()}\n",
    "\n",
    "print(\"\\n=== Monte Carlo Cross-Validation Results ===\")\n",
    "\n",
    "# Get sorted list of players by predicted success score\n",
    "sorted_players = sorted(final_actual_categories.keys(), key=lambda x: average_predicted_success_scores.get(x, 0), reverse=True)\n",
    "\n",
    "# Define how many players to show from the top and bottom\n",
    "num_to_show = 5 \n",
    "\n",
    "# Select the first and last few players\n",
    "top_players = sorted_players[:num_to_show]\n",
    "bottom_players = sorted_players[-num_to_show:]\n",
    "\n",
    "# top players\n",
    "print(f\"\\n--- Top {num_to_show} Players ---\")\n",
    "for player in top_players:\n",
    "    print(f\"{player}:\")\n",
    "    print(f\"   - Actual Category: {final_actual_categories.get(player, 'Unknown')}\")\n",
    "    print(f\"   - Predicted Category: {final_predicted_categories.get(player, 'Unknown')}\")\n",
    "    print(f\"   - Average Predicted Success Score: {average_predicted_success_scores.get(player, 'N/A'):.4f}\")\n",
    "\n",
    "# separator \n",
    "print(\"\\n... (skipping middle rows) ...\")\n",
    "\n",
    "# bottom players\n",
    "print(f\"\\n--- Bottom {num_to_show} Players ---\")\n",
    "for player in bottom_players:\n",
    "    print(f\"{player}:\")\n",
    "    print(f\"   - Actual Category: {final_actual_categories.get(player, 'Unknown')}\")\n",
    "    print(f\"   - Predicted Category: {final_predicted_categories.get(player, 'Unknown')}\")\n",
    "    print(f\"   - Average Predicted Success Score: {average_predicted_success_scores.get(player, 'N/A'):.4f}\")\n",
    "\n",
    "# Print a summary of the total number of players per category\n",
    "print(\"\\n--- Category Distribution Summary ---\")\n",
    "actual_counts = Counter(final_actual_categories.values())\n",
    "predicted_counts = Counter(final_predicted_categories.values())\n",
    "\n",
    "print(\"\\nActual Category Distribution:\")\n",
    "for category, count in actual_counts.items():\n",
    "    print(f\"  {category}: {count} players\")\n",
    "\n",
    "print(\"\\nPredicted Category Distribution:\")\n",
    "for category, count in predicted_counts.items():\n",
    "    print(f\"  {category}: {count} players\")\n",
    "\n",
    "print(\"\\n=== End of Monte Carlo Results ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
