{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4de839-0509-4921-8e2e-2226766cd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e418862-87fb-4433-8a45-a3bcba84abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nflRec_avg_normalized  nflYds_avg_normalized  nflTD_avg_normalized  \\\n",
      "0                0.155839               0.604536              0.268822   \n",
      "1                1.086216               1.264204              0.484466   \n",
      "2                2.806535               2.790986              2.856551   \n",
      "3               -0.074561              -0.142834             -0.270289   \n",
      "4                2.727540               3.277891              2.856551   \n",
      "..                    ...                    ...                   ...   \n",
      "93              -0.648732              -0.536752             -0.090585   \n",
      "94              -0.721875              -0.767983             -0.809399   \n",
      "95              -0.663361              -0.625941             -0.629695   \n",
      "96              -0.912046              -0.890204             -0.809399   \n",
      "97              -0.656046              -0.652367             -0.539844   \n",
      "\n",
      "    AP1_avg_normalized  St_avg_normalized  PB_avg_normalized  \n",
      "0            -0.205177           1.040796          -0.226919  \n",
      "1            -0.205177           1.402277          -0.226919  \n",
      "2             4.525963           2.125240           4.454775  \n",
      "3            -0.205177           1.040796          -0.226919  \n",
      "4             4.525963           2.125240           4.454775  \n",
      "..                 ...                ...                ...  \n",
      "93           -0.205177          -0.766611          -0.226919  \n",
      "94           -0.205177          -0.766611          -0.226919  \n",
      "95           -0.205177          -0.766611          -0.226919  \n",
      "96           -0.205177          -0.766611          -0.226919  \n",
      "97           -0.205177          -0.766611          -0.226919  \n",
      "\n",
      "[98 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# File paths for the datasets\n",
    "file_paths_nfl = [\n",
    "    \"data/historical-nfl/2020wr.csv\",\n",
    "    \"data/historical-nfl/2021wr.csv\",\n",
    "    \"data/historical-nfl/2022wr.csv\"\n",
    "]\n",
    "\n",
    "# Load and combine the datasets\n",
    "combined_data_nfl = pd.concat([pd.read_csv(path) for path in file_paths_nfl], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_path_nfl = \"data/historical-nfl/combined_wr_data.csv\"\n",
    "combined_data_nfl.to_csv(output_path_nfl, index=False)\n",
    "\n",
    "# Load and preprocess nfl stats\n",
    "nfl_stats = pd.read_csv(\"data/historical-nfl/combined_wr_data.csv\")\n",
    "nfl_stats['nflYears'] = nfl_stats['nflYears'].astype(float)\n",
    "\n",
    "# List of columns to normalize and average\n",
    "nfl_metrics_to_normalize = ['nflRec', 'nflYds', 'nflTD', 'AP1', 'St', 'PB']\n",
    "\n",
    "# Average the metrics by dividing by nflYears\n",
    "for metric in nfl_metrics_to_normalize:\n",
    "    nfl_stats[metric + '_avg'] = nfl_stats[metric] / nfl_stats['nflYears']\n",
    "\n",
    "# Standardize the averaged metrics\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Generate the list of column names for the normalized columns\n",
    "normalized = [metric + '_avg_normalized' for metric in nfl_metrics_to_normalize]\n",
    "\n",
    "# Standardize the averaged metrics (normalize by standard deviation)\n",
    "nfl_stats[normalized] = scaler.fit_transform(nfl_stats[[metric + '_avg' for metric in nfl_metrics_to_normalize]])\n",
    "print(nfl_stats[normalized])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8381d6-2cd0-4afb-bedb-6d53db36a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the un-averaged metrics\n",
    "nfl_stats['wAV_normalized'] = scaler.fit_transform(nfl_stats[['wAV']])\n",
    "\n",
    "# Define success metric using the normalized averaged metrics\n",
    "nfl_stats['SuccessMetric'] = (nfl_stats['nflYds_avg_normalized'] + nfl_stats['nflTD_avg_normalized'] +\n",
    "                               nfl_stats['nflRec_avg_normalized'] + nfl_stats['AP1_avg_normalized'] +\n",
    "                               nfl_stats['St_avg_normalized'] + nfl_stats['PB_avg_normalized'] +\n",
    "                               nfl_stats['wAV_normalized'])\n",
    "\n",
    "\n",
    "# File paths for the datasets\n",
    "file_paths_measurements = [\n",
    "    \"data/historical-measurements/2020wr.csv\",\n",
    "    \"data/historical-measurements/2021wr.csv\",\n",
    "    \"data/historical-measurements/2022wr.csv\"\n",
    "]\n",
    "\n",
    "file_paths_combine = [\n",
    "    \"data/historical-combine/2020wr.csv\",\n",
    "    \"data/historical-combine/2021wr.csv\",\n",
    "    \"data/historical-combine/2022wr.csv\"\n",
    "]\n",
    "\n",
    "file_paths_college = [\n",
    "    \"data/historical-college/2020wr.csv\",\n",
    "    \"data/historical-college/2021wr.csv\",\n",
    "    \"data/historical-college/2022wr.csv\"\n",
    "]\n",
    "\n",
    "# Load and combine the datasets\n",
    "combined_data_measurements = pd.concat([pd.read_csv(path) for path in file_paths_measurements], ignore_index=True)\n",
    "combined_data_combine = pd.concat([pd.read_csv(path) for path in file_paths_combine], ignore_index=True)\n",
    "combined_data_college = pd.concat([pd.read_csv(path) for path in file_paths_college], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_path_measurements = \"data/historical-measurements/combined_wr_data.csv\"\n",
    "combined_data_measurements.to_csv(output_path_measurements, index=False)\n",
    "\n",
    "output_path_combine = \"data/historical-combine/combined_wr_data.csv\"\n",
    "combined_data_combine.fillna(np.nan, inplace=True)\n",
    "combined_data_combine.to_csv(output_path_combine, index=False)\n",
    "\n",
    "output_path_college = \"data/historical-college/combined_wr_data.csv\"\n",
    "combined_data_college.to_csv(output_path_college, index=False)\n",
    "\n",
    "# Select features and target variable\n",
    "measurements = pd.read_csv(\"data/historical-measurements/combined_wr_data.csv\")\n",
    "combine_stats = pd.read_csv(\"data/historical-combine/combined_wr_data.csv\")\n",
    "college_stats = pd.read_csv(\"data/historical-college/combined_wr_data.csv\")\n",
    "\n",
    "conference_rankings = {\n",
    "    'SEC': 10,\n",
    "    'Big Ten': 9,\n",
    "    'Big 12': 8,\n",
    "    'Pac-12': 7,\n",
    "    'ACC': 6,\n",
    "    'AAC': 5,\n",
    "    'MWC': 4,\n",
    "    'C-USA': 3,\n",
    "    'Ind': 2,\n",
    "    'CAA': 1\n",
    "}\n",
    "\n",
    "# Map conference names to their rankings, default to 1 for any conference not listed\n",
    "college_stats['ConfRank'] = college_stats['Conf'].map(conference_rankings).fillna(1)\n",
    "\n",
    "# Merge datasets on Player column\n",
    "data = college_stats.merge(measurements, on=\"Player\").merge(combine_stats, on=\"Player\")\n",
    "\n",
    "# Ensure that the players in the target and features match\n",
    "merged_data = data.merge(nfl_stats[['Player', 'SuccessMetric']], on='Player', how='inner')\n",
    "\n",
    "# Features\n",
    "features = merged_data[['Rec', 'Yds', 'Y/R', 'TD', 'Y/G', 'G', 'ConfRank', '40yd', 'Height(in)', 'Weight', 'Hand(in)', 'Arm(in)', 'Wingspan(in)']]\n",
    "                        \n",
    "                        #, 'Height(in)', 'Weight', 'Hand(in)', 'Arm(in)', 'Wingspan(in)']]                 \n",
    "\n",
    "# Normalize the feature metrics (standard deviation normalization)\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the normalized features back into a DataFrame with the same column names\n",
    "features_normalized_df = pd.DataFrame(features_normalized, columns=features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc801eb7-6881-463b-a3cf-12299977d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Player  SuccessMetric\n",
      "0    Henry Ruggs III       1.586875\n",
      "1        Jerry Jeudy       5.050368\n",
      "2        CeeDee Lamb      23.397997\n",
      "3       Jalen Reagor       0.255184\n",
      "4   Justin Jefferson      23.435530\n",
      "..               ...            ...\n",
      "91      Jalen Nailor      -3.173959\n",
      "92  Michael Woods II      -4.289741\n",
      "93         Bo Melton      -3.724292\n",
      "94      Dareke Young      -4.602134\n",
      "95      Samori Toure      -3.653553\n",
      "\n",
      "[96 rows x 2 columns]\n",
      "LinearRegression()\n",
      "[-0.0147337   0.0131408   0.21131013  0.48185992 -0.09552428 -1.06140596\n",
      "  0.64747054 -9.62386049 -0.35539656  0.05308733 -2.82538328  0.28456448\n",
      "  0.24282838]\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "target = merged_data['SuccessMetric']\n",
    "target2 = merged_data[['Player','SuccessMetric']]\n",
    "print(target2)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.7)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model)\n",
    "print(model.coef_)\n",
    "\n",
    "# Predict and categorize players in the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Reshape predictions to be a 2D array (n_samples, 1)\n",
    "predictions_reshaped = predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af954d7-7d0c-42a2-a8b6-96e9ce0d2bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.57317533e+00],\n",
       "       [ 3.19751212e+00],\n",
       "       [-6.61873321e+00],\n",
       "       [-8.18894811e-01],\n",
       "       [-2.00920347e+00],\n",
       "       [ 8.74893214e+00],\n",
       "       [ 6.64450799e+00],\n",
       "       [-2.20614817e-01],\n",
       "       [-1.94022760e+00],\n",
       "       [ 1.16340001e+00],\n",
       "       [-1.82638960e+00],\n",
       "       [ 9.42430708e+00],\n",
       "       [ 5.10043262e+00],\n",
       "       [ 5.54037266e+00],\n",
       "       [ 3.87104418e+00],\n",
       "       [-6.24792488e+00],\n",
       "       [ 2.07638712e+00],\n",
       "       [-8.18011596e+00],\n",
       "       [-8.26909994e-04],\n",
       "       [ 2.67651171e+00],\n",
       "       [ 5.79575693e+00],\n",
       "       [ 5.95631014e-02],\n",
       "       [-3.85148113e+00],\n",
       "       [ 2.92745336e+00],\n",
       "       [-3.01146014e+00],\n",
       "       [ 4.43231575e+00],\n",
       "       [-9.53477767e+00],\n",
       "       [-6.12393324e+00],\n",
       "       [ 1.09952741e+01],\n",
       "       [-3.08933744e-01],\n",
       "       [ 4.38566896e+00],\n",
       "       [-7.40378657e+00],\n",
       "       [ 1.36719642e+00],\n",
       "       [-2.32334881e+00],\n",
       "       [-4.91321900e-01],\n",
       "       [ 4.87167090e+00],\n",
       "       [-5.17403444e+00],\n",
       "       [ 1.14143961e+00],\n",
       "       [ 1.04880752e+00],\n",
       "       [-1.14563769e+01],\n",
       "       [-5.73994906e+00],\n",
       "       [-9.33570151e+00],\n",
       "       [ 3.78835765e+00],\n",
       "       [-1.31276443e+01],\n",
       "       [-6.78342912e+00],\n",
       "       [ 4.45482809e+00],\n",
       "       [ 6.64248395e-01],\n",
       "       [ 2.74222405e+00],\n",
       "       [ 2.88080689e+00],\n",
       "       [ 7.04345782e-01],\n",
       "       [-3.81435677e+00],\n",
       "       [-1.37463070e+00],\n",
       "       [ 3.57744998e+00],\n",
       "       [-5.13722076e+00],\n",
       "       [-5.53821272e+00],\n",
       "       [ 9.03252001e+00],\n",
       "       [ 9.84858974e+00],\n",
       "       [ 2.52589519e-01],\n",
       "       [-7.49244297e+00],\n",
       "       [ 3.34219447e+00],\n",
       "       [ 7.10845624e+00],\n",
       "       [-1.30424472e+00],\n",
       "       [-1.79385485e+00],\n",
       "       [-3.71898320e+00],\n",
       "       [-3.52788847e+00],\n",
       "       [ 1.32428290e+00],\n",
       "       [ 1.29984726e+00],\n",
       "       [ 3.06029022e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d06c1ca-e6a7-4033-ba41-062d1317388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 0, 4, 4, 0, 0, 0, 1, 0, 4, 0, 4, 1, 0, 2, 1, 0, 0, 4, 4,\n",
       "       0, 0, 0, 0, 0, 4, 2, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4,\n",
       "       0, 0, 4, 0, 0, 1, 0, 0, 4, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4, 0, 0, 1,\n",
       "       0, 1, 1, 4, 3, 4, 0, 1, 1, 4, 0, 4, 0, 1, 1, 1, 4, 3, 4, 3, 4, 4,\n",
       "       1, 4, 0, 4, 0, 1, 3, 3, 1, 4, 1, 3, 4, 0, 4, 1, 0, 4, 4, 3, 0, 3,\n",
       "       4, 3, 3, 1, 4, 4, 4, 4, 0, 0, 4, 0, 0, 1, 1, 4, 3, 4, 1, 0, 0, 0,\n",
       "       0, 4, 4, 4], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use kmeans to fit and predict categories for all players\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "kmeans\n",
    "cluster_labels = kmeans.fit_predict(np.concatenate((y_test.to_numpy().reshape(-1, 1), predictions_reshaped), axis=0))\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457a9e4-6f67-41ca-b63b-c9e1304f2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans.fit_predict(np.concatenate((y_test.to_numpy().reshape(-1, 1), predictions_reshaped), axis=0))\n",
    "\n",
    "# Separate cluster labels for predicted and actual categories\n",
    "num_predictions = len(predictions_reshaped)\n",
    "predicted_clusters = cluster_labels[num_predictions:]\n",
    "actual_clusters = cluster_labels[:len(y_test)]\n",
    "\n",
    "# Add success metrics to associate cluster with category\n",
    "y_combined = pd.concat([y_test.reset_index(drop=True), pd.Series(predictions)], axis=0)\n",
    "cluster_avg_scores = pd.DataFrame({\n",
    "    \"Cluster\": cluster_labels,\n",
    "    \"Score\": y_combined\n",
    "}).groupby(\"Cluster\")[\"Score\"].mean()\n",
    "\n",
    "# Sort clusters by average score\n",
    "sorted_clusters = cluster_avg_scores.sort_values(ascending=False)\n",
    "\n",
    "# Map sorted clusters to categories\n",
    "cluster_to_category = {\n",
    "    sorted_clusters.index[0]: \"All-Pro\",\n",
    "    sorted_clusters.index[1]: \"Pro Bowler\",\n",
    "    sorted_clusters.index[2]: \"Starter\",\n",
    "    sorted_clusters.index[3]: \"Backup\",\n",
    "    sorted_clusters.index[4]: \"Practice Squad\"\n",
    "}\n",
    "\n",
    "# Function to categorize player based on cluster\n",
    "def categorize_player_automated(cluster_label: int):\n",
    "    return cluster_to_category.get(cluster_label, \"Unknown\")\n",
    "\n",
    "# Apply the categorization\n",
    "predicted_categories = [categorize_player_automated(cluster) for cluster in predicted_clusters]\n",
    "actual_categories = [categorize_player_automated(cluster) for cluster in actual_clusters]\n",
    "\n",
    "# Calculate the distribution of predicted categories\n",
    "predicted_category_counts = Counter(predicted_categories)\n",
    "\n",
    "# Calculate the distribution of actual categories\n",
    "actual_category_counts = Counter(actual_categories)\n",
    "\n",
    "# Print the distributions in a readable format\n",
    "print(\"\\nPredicted Category Distribution:\")\n",
    "for category, count in predicted_category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "print(\"\\nActual Category Distribution:\")\n",
    "for category, count in actual_category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "# Output results with aligned columns\n",
    "for player, pred, actual, pred_category, actual_category in zip(\n",
    "    merged_data['Player'][X_test.index], \n",
    "    predictions, \n",
    "    y_test, \n",
    "    predicted_categories, \n",
    "    actual_categories\n",
    "):\n",
    "    print(f\"Player: {player:<20} Predicted Success: {pred:>10.2f}  Actual Success: {actual:>10.2f}  Predicted Category: {pred_category:<15} Actual Category: {actual_category:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
